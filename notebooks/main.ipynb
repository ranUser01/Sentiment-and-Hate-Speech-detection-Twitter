{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "impossible-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import nltk.tokenize\n",
    "import numpy\n",
    "import pandas\n",
    "import pickle\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "posted-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on Tokeniser demo code from LearnIT\n",
    "# line = 'A cat sat on the mat. His name was MÃ¥ns.'\n",
    "# with open('../data/offensive/train_text.txt', 'r') as f2:\n",
    "#     line = f2.read()\n",
    "    \n",
    "# def tokenize(line):\n",
    "#     # Initialise lists\n",
    "   \n",
    "#     tokens = []\n",
    "#     unmatchable = []\n",
    "\n",
    "#     # Compile patterns for speedup\n",
    "#     token_pat = re.compile(r'\\w+')\n",
    "#     skippable_pat = re.compile(r'\\s+')  # typically spaces\n",
    "\n",
    "#     # As long as there's any material left...\n",
    "#     while line:\n",
    "#         # Try finding a skippable token delimiter first.\n",
    "#         skippable_match = re.search(skippable_pat, line)\n",
    "#         if skippable_match and skippable_match.start() == 0:\n",
    "#             # If there is one at the beginning of the line, just skip it.\n",
    "#             line = line[skippable_match.end():]\n",
    "#         else:\n",
    "#             # Else try finding a real token.\n",
    "#             token_match = re.search(token_pat, line)\n",
    "#             if token_match and token_match.start() == 0:\n",
    "#                 # If there is one at the beginning of the line, casefold and tokenise it.\n",
    "#                 tokens.append(line[:token_match.end()].casefold())\n",
    "#                 line = line[token_match.end():]\n",
    "#             else:\n",
    "#                 # Else there is unmatchable material here.\n",
    "#                 # It ends where a skippable or token match starts, or at the end of the line.\n",
    "#                 unmatchable_end = len(line)\n",
    "#                 if skippable_match:\n",
    "#                     unmatchable_end = skippable_match.start()\n",
    "#                 if token_match:\n",
    "#                     unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "#                 # Add it to unmatchable and discard from line.\n",
    "#                 # unmatchable.append(line[:unmatchable_end])\n",
    "#                 line = line[unmatchable_end:]\n",
    "    \n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "signed-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on Frequency plots demo code from LearnIT\n",
    "def frequency(corpus):\n",
    "    # tok = nltk.tokenize.TreebankWordTokenizer()\n",
    "    #\n",
    "    # corpus = []\n",
    "    # with open('news-commentary-v16.en', 'r') as f:\n",
    "    #     for line in f:\n",
    "    #         corpus.extend(t for line in f for t in tok.tokenize(line))\n",
    "    #\n",
    "    # with open('ncv16-list.pkl', 'wb') as f:\n",
    "    #     pickle.dump(corpus, f)\n",
    "\n",
    "    with open('ncv16-list.pkl', 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "\n",
    "    voc = collections.Counter(corpus)\n",
    "    frq = pandas.DataFrame(voc.most_common(), columns=['token', 'frequency'])\n",
    "\n",
    "    # Index in the sorted list\n",
    "    frq['idx'] = frq.index + 1\n",
    "\n",
    "    # Frequency normalised by corpus size\n",
    "    frq['norm_freq'] = frq.frequency / len(corpus)\n",
    "\n",
    "    # Cumulative normalised frequency\n",
    "    frq['cumul_frq'] = frq.norm_freq.cumsum()\n",
    "\n",
    "    seaborn.set_theme(style='whitegrid')\n",
    "\n",
    "    # Plot: Cumulative frequency by index\n",
    "    seaborn.relplot(x='idx', y='cumul_frq', data=frq)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot: Cumulative frequency by index, top 10000 tokens\n",
    "    seaborn.relplot(x='idx', y='cumul_frq', data=frq[:10000], kind='line')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot: Log-log plot for Zipf's law\n",
    "    frq['log_frq'] = numpy.log(frq.frequency)\n",
    "    frq['log_rank'] = numpy.log(frq.frequency.rank(ascending=False))\n",
    "    seaborn.relplot(x='log_rank', y='log_frq', data=frq)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a413113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "with open('../data/offensive/train_text.txt', 'r') as f2:\n",
    "    line = f2.read()\n",
    "\n",
    "tokens = tk.tokenize(line)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13f24e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300884\n"
     ]
    }
   ],
   "source": [
    "#Tokens\n",
    "toklen = len(tokens)\n",
    "print(toklen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86c257c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24707\n"
     ]
    }
   ],
   "source": [
    "#TYPES\n",
    "cnt = Counter()\n",
    "typetokens = len(Counter(tokens))\n",
    "print(typetokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "329d8b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08211470201140639\n"
     ]
    }
   ],
   "source": [
    "#type/tokenratio\n",
    "print(typetokens/toklen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85385009",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sentiment/train_text.txt', 'r') as f3:\n",
    "    datasen = f3.read()\n",
    "tokensen = tk.tokenize(datasen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa15cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032721\n"
     ]
    }
   ],
   "source": [
    "#Tokens\n",
    "toklen1 = len(tokensen)\n",
    "print(toklen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c10a73ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62257\n"
     ]
    }
   ],
   "source": [
    "#TYPES\n",
    "typetokens1 = len(Counter(tokensen))\n",
    "print(typetokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bef6b08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06028443306565859\n"
     ]
    }
   ],
   "source": [
    "#type/tokenratio\n",
    "print(typetokens1/toklen1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54e2dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/GreatGatsby.txt', 'r') as f4:\n",
    "    line1 = f4.read()\n",
    "\n",
    "tokens2 = tk.tokenize(line1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0271da4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64835\n"
     ]
    }
   ],
   "source": [
    "toklen2 = len(tokens2)\n",
    "print(toklen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd834840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6833\n"
     ]
    }
   ],
   "source": [
    "#TYPES\n",
    "typetokens2 = len(Counter(tokens2))\n",
    "print(typetokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2953a5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10539060692527184\n"
     ]
    }
   ],
   "source": [
    "#type/tokenratio\n",
    "print(typetokens2/toklen2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6c96eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267520\n",
      "69920\n"
     ]
    }
   ],
   "source": [
    "#OFFENSIVE\n",
    "with open('../data/offensive/train_text.txt', 'r') as f2:\n",
    "    line = f2.read()\n",
    "\n",
    "# Initialise lists\n",
    "tokens = []\n",
    "unmatchable = []\n",
    "\n",
    "# Compile patterns for speedup\n",
    "token_pat = re.compile(r'\\w+')\n",
    "skippable_pat = re.compile(r'\\s+')  # typically spaces\n",
    "\n",
    "# As long as there's any material left...\n",
    "while line:\n",
    "    # Try finding a skippable token delimiter first.\n",
    "    skippable_match = re.search(skippable_pat, line)\n",
    "    if skippable_match and skippable_match.start() == 0:\n",
    "        # If there is one at the beginning of the line, just skip it.\n",
    "        line = line[skippable_match.end():]\n",
    "    else:\n",
    "        # Else try finding a real token.\n",
    "        token_match = re.search(token_pat, line)\n",
    "        if token_match and token_match.start() == 0:\n",
    "            # If there is one at the beginning of the line, tokenise it.\n",
    "            tokens.append(line[:token_match.end()])\n",
    "            line = line[token_match.end():]\n",
    "        else:\n",
    "            # Else there is unmatchable material here.\n",
    "            # It ends where a skippable or token match starts, or at the end of the line.\n",
    "            unmatchable_end = len(line)\n",
    "            if skippable_match:\n",
    "                unmatchable_end = skippable_match.start()\n",
    "            if token_match:\n",
    "                unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "            # Add it to unmatchable and discard from line.\n",
    "            unmatchable.append(line[:unmatchable_end])\n",
    "            line = line[unmatchable_end:]\n",
    "\n",
    "print(len(tokens))\n",
    "print(len(unmatchable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91fa1049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user', 29812),\n",
       " ('the', 7271),\n",
       " ('is', 5656),\n",
       " ('to', 5472),\n",
       " ('a', 4582),\n",
       " ('and', 4106),\n",
       " ('you', 3683),\n",
       " ('of', 3347),\n",
       " ('are', 3076),\n",
       " ('I', 3075),\n",
       " ('that', 2369),\n",
       " ('in', 2321),\n",
       " ('s', 2170),\n",
       " ('for', 2153),\n",
       " ('t', 2018),\n",
       " ('it', 1828),\n",
       " ('he', 1681),\n",
       " ('on', 1475),\n",
       " ('she', 1421),\n",
       " ('with', 1299)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e250c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment\n",
    "with open('../data/sentiment/train_text.txt', 'r') as f3:\n",
    "    datasen = f3.read()\n",
    "\n",
    "# Initialise lists\n",
    "tokens1 = []\n",
    "unmatchable1 = []\n",
    "\n",
    "# Compile patterns for speedup\n",
    "token_pat1 = re.compile(r'\\w+')\n",
    "skippable_pat1 = re.compile(r'\\s+')  # typically spaces\n",
    "\n",
    "# As long as there's any material left...\n",
    "while datasen:\n",
    "    # Try finding a skippable token delimiter first.\n",
    "    skippable_match1 = re.search(skippable_pat1, datasen)\n",
    "    if skippable_match1 and skippable_match1.start() == 0:\n",
    "        # If there is one at the beginning of the line, just skip it.\n",
    "        datasen = datasen[skippable_match1.end():]\n",
    "    else:\n",
    "        # Else try finding a real token.\n",
    "        token_match1 = re.search(token_pat1, datasen)\n",
    "        if token_match1 and token_match1.start() == 0:\n",
    "            # If there is one at the beginning of the line, tokenise it.\n",
    "            tokens1.append(datasen[:token_match1.end()])\n",
    "            datasen = datasen[token_match1.end():]\n",
    "        else:\n",
    "            # Else there is unmatchable material here.\n",
    "            # It ends where a skippable or token match starts, or at the end of the line.\n",
    "            unmatchable_end1 = len(datasen)\n",
    "            if skippable_match1:\n",
    "                unmatchable_end1 = skippable_match1.start()\n",
    "            if token_match1:\n",
    "                unmatchable_end1 = min(unmatchable_end1, token_match1.start())\n",
    "            # Add it to unmatchable and discard from line.\n",
    "            unmatchable1.append(datasen[:unmatchable_end1])\n",
    "            datasen = datasen[unmatchable_end1:]\n",
    "\n",
    "print(len(tokens1))\n",
    "print(len(unmatchable1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ed19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tokens1).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae208e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GREATGATSBY\n",
    "with open('../data/GreatGatsby.txt', 'r') as f4:\n",
    "    line1 = f4.read()\n",
    "\n",
    "# Initialise lists\n",
    "tokens2 = []\n",
    "unmatchable2 = []\n",
    "\n",
    "# Compile patterns for speedup\n",
    "token_pat2 = re.compile(r'\\w+')\n",
    "skippable_pat2 = re.compile(r'\\s+')  # typically spaces\n",
    "\n",
    "# As long as there's any material left...\n",
    "while line1:\n",
    "    # Try finding a skippable token delimiter first.\n",
    "    skippable_match2 = re.search(skippable_pat2, line1)\n",
    "    if skippable_match2 and skippable_match2.start() == 0:\n",
    "        # If there is one at the beginning of the line, just skip it.\n",
    "        line1 = line1[skippable_match2.end():]\n",
    "    else:\n",
    "        # Else try finding a real token.\n",
    "        token_match2 = re.search(token_pat2, line1)\n",
    "        if token_match2 and token_match2.start() == 0:\n",
    "            # If there is one at the beginning of the line, tokenise it.\n",
    "            tokens2.append(line1[:token_match2.end()])\n",
    "            line1 = line1[token_match2.end():]\n",
    "        else:\n",
    "            # Else there is unmatchable material here.\n",
    "            # It ends where a skippable or token match starts, or at the end of the line.\n",
    "            unmatchable_end2 = len(line1)\n",
    "            if skippable_match2:\n",
    "                unmatchable_end2 = skippable_match2.start()\n",
    "            if token_match2:\n",
    "                unmatchable_end2 = min(unmatchable_end2, token_match2.start())\n",
    "            # Add it to unmatchable and discard from line.\n",
    "            unmatchable2.append(line1[:unmatchable_end2])\n",
    "            line1 = line1[unmatchable_end2:]\n",
    "\n",
    "print(len(tokens2))\n",
    "print(len(unmatchable2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b79e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27f213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
